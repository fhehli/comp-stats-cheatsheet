\section{Bagging and Boosting}\label{sec:bagging_and_boosting}
\begin{sectionbox}[Bagging and Subbagging]\nospacing{}
  \textbf{B}ootstrap \textbf{agg}regat\textbf{ing} (bagging) (mostly on trees), uses $\hat g(\cdot): \mathbb{R}^{p}\to \mathbb{R}$ and ensembles them (which comes at the loss of interpretability).
  \begin{enumeratenosep}[label=\roman*]
    \item Generate bootstrap sample $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{n}^{\ast}, Y_{n}^{\ast})$ and compute $\hat{g}^{\ast}_{i=1}(\cdot)$. Repeat $B$ times.
    \item Aggregate bootstrap estimates with $\hat{g}_{\textrm Bag}(\cdot) = B^{-1}\sum_{i=1}^{B} \hat{g}^{\ast}_{i}(\cdot) \approx \mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)]$.
  \end{enumeratenosep}
  Note that $\hat{g}_{\textrm Bag}(\cdot) = \hat{g}(\cdot) + \underbrace{(\mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)] -\hat{g}(\cdot))}_{\text{bootstrap bias estimate}}$.
  We can reduce variance at price of higher bias (at least for trees).
  In fact, for many $x$, $Var(\hat{g}_{\textrm Bag}(x)) < Var(\hat{g}(x))$. We can use larger trees (higher variance) to balance the bias-variance trade-off.

  For \textbf{Sub}sample \textbf{agg}regat\textbf{ing} (Subbagging), we draw $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{m}^{\ast}, Y_{m}^{\ast})$ without replacement (e.g. with $m = \lfloor n/2\rfloor$), which can be cheaper overall and is equivalent to Bagging in some simple settings.
\end{sectionbox}

\begin{sectionbox}[$L_{2}$Boosting]\nospacing{}
  Similar to Bagging, iterates on a ``base-learner'' by continually adding a fit on the residuals.
  \begin{enumeratenosep}[label=\roman*]
    \item Get first fit $\hat{g}_{1}(\cdot)$ by fitting on the full data. Compute residuals $U_{i} = Y_{i} - \hat{g}_{1}(X_{i})$ and let $\hat{f}_{1}(\cdot) = \nu \hat{g}_{1}(\cdot)$ with $0 < \nu \leq 1$ (typically $\nu = 0.1$).
    \item For $m = 2, 3, \dots, M$ fit $\hat{g}_{m}(\cdot)$ on residuals $U_{i}$ and set $\hat{f}_{m}(\cdot) = \hat{f}_{m-1}(\cdot) + \nu \hat{g}_{m}(\cdot)$ (and update residuals using $\hat{f}_{m}(\cdot)$).
  \end{enumeratenosep}
  The main tuning parameter is the stopping point $M$. Boosting \emph{increases} the bias and can be used to ensemble trees to fit more complex data. See e.g.
  \begin{mintlinebox}{R}
    ?mboost; ?xgboost; ?gbm;
  \end{mintlinebox}
\end{sectionbox}
